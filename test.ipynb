{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b6e445-0cfd-42ee-a319-3d3ec4397224",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "import torch_geometric\n",
    "print(torch_geometric.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "262f876a-1ff5-4ac3-a613-f4de484a2c3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_loader.py\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "\n",
    "class SubjectDataset(Dataset):\n",
    "    def __init__(self, high_dim_features, low_dim_features, labels):#,edge_index):\n",
    "        # 由于我们将在模型中处理低维特征的嵌入，所以我们在这里不需要转换为Tensor\n",
    "        self.high_dim_features = high_dim_features\n",
    "        self.low_dim_features = low_dim_features\n",
    "        self.labels = labels\n",
    "        # self.edge_index = edge_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        high_dim_sample = torch.tensor(self.high_dim_features.iloc[idx].values, dtype=torch.float)\n",
    "        low_dim_sample = self.low_dim_features.iloc[idx].values  # 作为NumPy数组保持\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return high_dim_sample, low_dim_sample, label #,self.edge_index\n",
    "\n",
    "def create_full_connected_edge_index(num_nodes):\n",
    "    # 生成一个全连接网络的邻接矩阵\n",
    "    adj_matrix = torch.ones((num_nodes, num_nodes)) - torch.eye(num_nodes)\n",
    "    # 将邻接矩阵转换为边索引\n",
    "    edge_index, _ = dense_to_sparse(adj_matrix)\n",
    "    return edge_index\n",
    "\n",
    "def load_and_align_data(high_dim_path, low_dim_path, labels_path, test_size=0.2, val_size = 0.1,random_state=42):\n",
    "    # 加载数据\n",
    "    high_dim_df = pd.read_csv(high_dim_path)\n",
    "    low_dim_df = pd.read_csv(low_dim_path)\n",
    "    labels_df = pd.read_csv(labels_path)\n",
    "\n",
    "    # 假设第一列是subject_id，对齐数据\n",
    "    merged_df = high_dim_df.merge(low_dim_df, on='PTID',how = 'inner').merge(labels_df, on='PTID',how = 'inner')\n",
    "\n",
    "    # 假设'high_dim_data'和'low_dim_data'分别是包含在您的CSV文件列名中的高维和低维数据标识\n",
    "    high_dim_cols = [col for col in merged_df.columns if 'high_' in col]\n",
    "    low_dim_cols = [col for col in merged_df.columns if 'low_' in col]\n",
    "    labels = merged_df['label_DX'].values\n",
    "\n",
    "    # 分离出高维和低维特征\n",
    "    high_dim_features = merged_df[high_dim_cols]\n",
    "    low_dim_features = merged_df[low_dim_cols]\n",
    "\n",
    "    # 划分训练集和测试集\n",
    "    initial_train_high, test_high, initial_train_low, test_low, initial_train_labels, test_labels = train_test_split(\n",
    "        high_dim_features, low_dim_features, labels, test_size=test_size, random_state=random_state, stratify=labels)\n",
    "    \n",
    "     # 进一步划分出验证集\n",
    "    train_high, val_high, train_low, val_low, train_labels, val_labels = train_test_split(\n",
    "        initial_train_high, initial_train_low, initial_train_labels, test_size=val_size, random_state=random_state, stratify=initial_train_labels)\n",
    "\n",
    "    # 在这里调用 create_full_connected_edge_index 函数来创建 edge_index\n",
    "    # num_nodes = len(merged_df)\n",
    "    # edge_index = create_full_connected_edge_index(num_nodes)\n",
    "\n",
    "    # 创建数据集实例\n",
    "    train_dataset = SubjectDataset(train_high, train_low, train_labels)#, edge_index)\n",
    "    val_dataset = SubjectDataset(val_high, val_low, val_labels)#, edge_index)\n",
    "    test_dataset = SubjectDataset(test_high, test_low, test_labels)#, edge_index)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "def create_data_loader(dataset, batch_size=32, shuffle=True):\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed3a3c6c-acfb-491d-b02e-488d239de99b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/AD_Classification'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4779f6a8-1129-4a82-8555-5b6c9e26bad0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1019090])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "os.getcwd()\n",
    "os.listdir()\n",
    "#os.getcwd()\n",
    "\n",
    "df = pd.read_csv(\"data/FINAL_MRI_DATA1.csv\")\n",
    "\n",
    "df\n",
    "\n",
    "def create_full_connected_edge_index(num_nodes):\n",
    "    # 生成一个全连接网络的邻接矩阵\n",
    "    adj_matrix = torch.ones((num_nodes, num_nodes)) - torch.eye(num_nodes)\n",
    "    # 将邻接矩阵转换为边索引\n",
    "    edge_index, _ = dense_to_sparse(adj_matrix)\n",
    "    return edge_index\n",
    "\n",
    "create_full_connected_edge_index(1010).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2b3a6b4-a379-439c-a4db-7805b12855af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "727\n",
      "81\n",
      "Sample 0 - High dim features: torch.Size([498]), Low dim features: (17,), Label: 0\n",
      "Sample 1 - High dim features: torch.Size([498]), Low dim features: (17,), Label: 1\n",
      "Sample 2 - High dim features: torch.Size([498]), Low dim features: (17,), Label: 0\n",
      "Batch 0 - High dim batch shape: torch.Size([2, 498]), Low dim batch shape: 2, Labels in batch: tensor([0, 0])\n",
      "Batch 1 - High dim batch shape: torch.Size([2, 498]), Low dim batch shape: 2, Labels in batch: tensor([0, 1])\n",
      "Batch 2 - High dim batch shape: torch.Size([2, 498]), Low dim batch shape: 2, Labels in batch: tensor([1, 0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 示例用法\n",
    "high_dim_path = 'data/FINAL_MRI_DATA1.csv'\n",
    "low_dim_path = 'data/FINAL_TABLEDATA_MRI.csv'\n",
    "labels_path = 'data/Y_label_NEW_01.csv'\n",
    "\n",
    "\n",
    "train_dataset, test_dataset ,validation_dataset= load_and_align_data(high_dim_path, low_dim_path, labels_path)\n",
    "train_loader = create_data_loader(train_dataset)\n",
    "test_loader = create_data_loader(test_dataset, shuffle=False)\n",
    "validation_loader = create_data_loader(validation_dataset, batch_size=2)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "\n",
    "# 测试一些样本\n",
    "for i in range(3):\n",
    "    high, low, label = train_dataset[i]\n",
    "    print(f\"Sample {i} - High dim features: {high.shape}, Low dim features: {low.shape}, Label: {label}\")\n",
    "    \n",
    "\n",
    "for batch_idx, (high, low, label) in enumerate(validation_loader):\n",
    "    print(f\"Batch {batch_idx} - High dim batch shape: {high.shape}, Low dim batch shape: {len(low)}, Labels in batch: {label}\")\n",
    "    if batch_idx == 2:  # 只测试前几个批次\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "53e15fa9-3339-48c1-abd0-c53809e30bba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/AD_Classification/data'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6a42c29-3866-4ea4-af33-a93d04aafd30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# config.py\n",
    "\n",
    "config = {\n",
    "  \"data\": {\n",
    "    \"high_dim_path\": 'data/FINAL_MRI_DATA1.csv',\n",
    "    \"low_dim_path\": 'data/FINAL_TABLEDATA_MRI.csv',\n",
    "    \"labels_path\": 'data/Y_label_NEW_01.csv',\n",
    "    \"batch_size\": 128,\n",
    "    \"shuffle\": True,\n",
    "    \"test_size\":0.1,\n",
    "    \"val_size\":0.1,\n",
    "    \"random_state\":12345\n",
    "  },\n",
    "  \"model\": {\n",
    "    \"type\": \"GCN\",\n",
    "    \"high_dim_input_size\": 498,  \n",
    "    \"low_dim_input_size\":17,\n",
    "    \"embedding_dim\":56,\n",
    "    \"output_dim\":2,\n",
    "    \"hidden_channels\":128,\n",
    "    \"num_heads\":4 \n",
    "  },\n",
    "  \"train\": {\n",
    "    \"epochs\": 250,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"device\": \"cuda:1\"\n",
    "  },\n",
    "  \"earlystopping\":{\n",
    "    \"patience\":5,\n",
    "    \"delta\":0.01\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58bc1e8e-cac6-4d47-9adc-a64728279d62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 14870.695577366012, Train Acc: 50.488997555012226, Val Loss: 1022416.5, Val Acc: 36.26373626373626\n",
      "Epoch 2, Train Loss: 47223.46878794261, Train Acc: 52.68948655256724, Val Loss: 1792125.5, Val Acc: 36.26373626373626\n",
      "Epoch 3, Train Loss: 1892.0467991062574, Train Acc: 42.665036674816626, Val Loss: 2065283.375, Val Acc: 36.26373626373626\n",
      "Epoch 4, Train Loss: 24170.352928715092, Train Acc: 63.81418092909536, Val Loss: 2407888.0, Val Acc: 36.26373626373626\n",
      "Epoch 5, Train Loss: 50941.06322274038, Train Acc: 63.93643031784841, Val Loss: 2632141.25, Val Acc: 36.26373626373626\n",
      "Epoch 6, Train Loss: 14061.340278523308, Train Acc: 61.36919315403423, Val Loss: 2716091.0, Val Acc: 36.26373626373626\n",
      "Epoch 7, Train Loss: 0.6712416069848197, Train Acc: 57.21271393643032, Val Loss: 2579112.25, Val Acc: 36.26373626373626\n",
      "Epoch 8, Train Loss: 0.6531868066106524, Train Acc: 63.93643031784841, Val Loss: 2717069.25, Val Acc: 36.26373626373626\n",
      "Epoch 9, Train Loss: 0.6642505867140633, Train Acc: 63.93643031784841, Val Loss: 2713623.25, Val Acc: 36.26373626373626\n",
      "Epoch 10, Train Loss: 11465.574448866504, Train Acc: 63.93643031784841, Val Loss: 2540775.75, Val Acc: 36.26373626373626\n",
      "Epoch 11, Train Loss: 0.6603819983346122, Train Acc: 63.93643031784841, Val Loss: 2511637.25, Val Acc: 36.26373626373626\n",
      "Epoch 12, Train Loss: 0.6589464374950954, Train Acc: 63.93643031784841, Val Loss: 2497028.0, Val Acc: 36.26373626373626\n",
      "Epoch 13, Train Loss: 0.6501390423093524, Train Acc: 63.93643031784841, Val Loss: 2487800.75, Val Acc: 36.26373626373626\n",
      "Epoch 14, Train Loss: 0.660797689642225, Train Acc: 63.93643031784841, Val Loss: 2485646.75, Val Acc: 36.26373626373626\n",
      "Epoch 15, Train Loss: 0.6567308051245553, Train Acc: 63.93643031784841, Val Loss: 2484439.5, Val Acc: 36.26373626373626\n",
      "Epoch 16, Train Loss: 0.6555438382284982, Train Acc: 63.93643031784841, Val Loss: 2483059.5, Val Acc: 36.26373626373626\n",
      "Epoch 17, Train Loss: 0.6529289994921003, Train Acc: 63.93643031784841, Val Loss: 2481726.5, Val Acc: 36.26373626373626\n",
      "Epoch 18, Train Loss: 0.6514968957219806, Train Acc: 63.93643031784841, Val Loss: 2480207.75, Val Acc: 36.26373626373626\n",
      "Epoch 19, Train Loss: 0.6611304794039045, Train Acc: 63.93643031784841, Val Loss: 2478635.5, Val Acc: 36.26373626373626\n",
      "Epoch 20, Train Loss: 0.6602339914866856, Train Acc: 63.93643031784841, Val Loss: 2478457.75, Val Acc: 36.26373626373626\n",
      "Epoch 21, Train Loss: 0.6637973444802421, Train Acc: 63.81418092909536, Val Loss: 2478290.5, Val Acc: 36.26373626373626\n",
      "Epoch 22, Train Loss: 0.6512490510940552, Train Acc: 63.93643031784841, Val Loss: 2478195.25, Val Acc: 36.26373626373626\n",
      "Epoch 23, Train Loss: 0.6535841396876744, Train Acc: 63.93643031784841, Val Loss: 2478382.5, Val Acc: 36.26373626373626\n",
      "Epoch 24, Train Loss: 0.6609887054988316, Train Acc: 63.93643031784841, Val Loss: 2478505.75, Val Acc: 36.26373626373626\n",
      "Epoch 25, Train Loss: 511.0877832514899, Train Acc: 63.93643031784841, Val Loss: 2479281.25, Val Acc: 36.26373626373626\n",
      "Epoch 26, Train Loss: 0.6574756247656686, Train Acc: 63.93643031784841, Val Loss: 2480320.75, Val Acc: 36.26373626373626\n",
      "Epoch 27, Train Loss: 0.6604588457516262, Train Acc: 63.93643031784841, Val Loss: 2480797.25, Val Acc: 36.26373626373626\n",
      "Epoch 28, Train Loss: 0.6538464171545846, Train Acc: 63.93643031784841, Val Loss: 2480999.25, Val Acc: 36.26373626373626\n",
      "Epoch 29, Train Loss: 0.6554852894374302, Train Acc: 63.93643031784841, Val Loss: 2481078.75, Val Acc: 36.26373626373626\n",
      "Epoch 30, Train Loss: 0.6556914022990635, Train Acc: 63.93643031784841, Val Loss: 2481101.5, Val Acc: 36.26373626373626\n",
      "Epoch 31, Train Loss: 747.4972663436617, Train Acc: 61.49144254278728, Val Loss: 2479948.75, Val Acc: 36.26373626373626\n",
      "Epoch 32, Train Loss: 0.6741088884217399, Train Acc: 63.93643031784841, Val Loss: 2479436.25, Val Acc: 36.26373626373626\n",
      "Epoch 33, Train Loss: 0.6535367710249764, Train Acc: 63.93643031784841, Val Loss: 2479192.0, Val Acc: 36.26373626373626\n",
      "Epoch 34, Train Loss: 0.6593020217759269, Train Acc: 63.93643031784841, Val Loss: 2479055.0, Val Acc: 36.26373626373626\n",
      "Epoch 35, Train Loss: 0.6579031773975917, Train Acc: 63.93643031784841, Val Loss: 2478977.25, Val Acc: 36.26373626373626\n",
      "Epoch 36, Train Loss: 2596.1750349743024, Train Acc: 63.93643031784841, Val Loss: 2478936.5, Val Acc: 36.26373626373626\n",
      "Epoch 37, Train Loss: 0.6545884438923427, Train Acc: 63.93643031784841, Val Loss: 2478895.5, Val Acc: 36.26373626373626\n",
      "Epoch 38, Train Loss: 0.659154610974448, Train Acc: 63.93643031784841, Val Loss: 2478885.0, Val Acc: 36.26373626373626\n",
      "Epoch 39, Train Loss: 0.654545818056379, Train Acc: 63.93643031784841, Val Loss: 2478882.0, Val Acc: 36.26373626373626\n",
      "Epoch 40, Train Loss: 0.667109625680106, Train Acc: 63.93643031784841, Val Loss: 2478877.25, Val Acc: 36.26373626373626\n",
      "Epoch 41, Train Loss: 0.6573908584458488, Train Acc: 63.93643031784841, Val Loss: 2478875.0, Val Acc: 36.26373626373626\n",
      "Epoch 42, Train Loss: 0.6567518200193133, Train Acc: 63.93643031784841, Val Loss: 2478870.25, Val Acc: 36.26373626373626\n",
      "Epoch 43, Train Loss: 0.6562697206224714, Train Acc: 63.93643031784841, Val Loss: 2478867.75, Val Acc: 36.26373626373626\n",
      "Epoch 44, Train Loss: 0.6568712762423924, Train Acc: 63.93643031784841, Val Loss: 2478868.25, Val Acc: 36.26373626373626\n",
      "Epoch 45, Train Loss: 0.6551811609949384, Train Acc: 63.93643031784841, Val Loss: 2478867.0, Val Acc: 36.26373626373626\n",
      "Epoch 46, Train Loss: 0.6528139369828361, Train Acc: 63.93643031784841, Val Loss: 2478867.0, Val Acc: 36.26373626373626\n",
      "Epoch 47, Train Loss: 0.6575064318520683, Train Acc: 63.93643031784841, Val Loss: 2478866.5, Val Acc: 36.26373626373626\n",
      "Epoch 48, Train Loss: 2540.6286414010183, Train Acc: 63.93643031784841, Val Loss: 2478892.25, Val Acc: 36.26373626373626\n",
      "Epoch 49, Train Loss: 2752.109476642949, Train Acc: 63.93643031784841, Val Loss: 2478905.25, Val Acc: 36.26373626373626\n",
      "Epoch 50, Train Loss: 0.6644307715552193, Train Acc: 63.93643031784841, Val Loss: 2478907.0, Val Acc: 36.26373626373626\n",
      "Epoch 51, Train Loss: 0.6573324373790196, Train Acc: 63.93643031784841, Val Loss: 2478907.0, Val Acc: 36.26373626373626\n",
      "Epoch 52, Train Loss: 0.6581149101257324, Train Acc: 63.93643031784841, Val Loss: 2478908.0, Val Acc: 36.26373626373626\n",
      "Epoch 53, Train Loss: 0.6530248437608991, Train Acc: 63.93643031784841, Val Loss: 2478908.0, Val Acc: 36.26373626373626\n",
      "Epoch 54, Train Loss: 0.6656137193952288, Train Acc: 63.93643031784841, Val Loss: 2478908.0, Val Acc: 36.26373626373626\n",
      "Epoch 55, Train Loss: 0.6555514761379787, Train Acc: 63.93643031784841, Val Loss: 2478908.0, Val Acc: 36.26373626373626\n",
      "Epoch 56, Train Loss: 0.6537470306668963, Train Acc: 63.93643031784841, Val Loss: 2478908.0, Val Acc: 36.26373626373626\n",
      "Epoch 57, Train Loss: 13453.175114095211, Train Acc: 63.93643031784841, Val Loss: 2478905.5, Val Acc: 36.26373626373626\n",
      "Epoch 58, Train Loss: 0.6543498379843575, Train Acc: 63.93643031784841, Val Loss: 2478902.0, Val Acc: 36.26373626373626\n",
      "Epoch 59, Train Loss: 3710.0058454871178, Train Acc: 63.93643031784841, Val Loss: 2478892.75, Val Acc: 36.26373626373626\n",
      "Epoch 60, Train Loss: 0.6558151159967694, Train Acc: 63.93643031784841, Val Loss: 2478891.75, Val Acc: 36.26373626373626\n",
      "Epoch 61, Train Loss: 0.6554677656718663, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 62, Train Loss: 0.6584947279521397, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 63, Train Loss: 0.659756600856781, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 64, Train Loss: 0.6551963261195591, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 65, Train Loss: 0.651686259678432, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 66, Train Loss: 0.6569840822901044, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 67, Train Loss: 0.6559766956738063, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 68, Train Loss: 0.6606782249041966, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 69, Train Loss: 0.6551703980990818, Train Acc: 63.691931540342296, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 70, Train Loss: 0.6631739480154855, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 71, Train Loss: 0.6523939115660531, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 72, Train Loss: 0.6611956528254918, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 73, Train Loss: 0.6499551023755755, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 74, Train Loss: 0.6573040229933602, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 75, Train Loss: 0.658874877861568, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 76, Train Loss: 0.6456545506204877, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 77, Train Loss: 0.6503828593662807, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 78, Train Loss: 0.6558652094432286, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 79, Train Loss: 0.6527214220591954, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 80, Train Loss: 0.6550571663039071, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 81, Train Loss: 0.6594589608056205, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 82, Train Loss: 0.6563783713749477, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 83, Train Loss: 0.6652775832584926, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 84, Train Loss: 11101.164852968284, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 85, Train Loss: 0.6558871269226074, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 86, Train Loss: 0.6536773187773568, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 87, Train Loss: 0.6619382926395961, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 88, Train Loss: 0.6559011084692818, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 89, Train Loss: 0.6626347388539996, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 90, Train Loss: 0.6590873428753444, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 91, Train Loss: 0.6548444628715515, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 92, Train Loss: 10381.025862004075, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 93, Train Loss: 0.6483272910118103, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 94, Train Loss: 0.6526893207005092, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 95, Train Loss: 0.6467876945223127, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 96, Train Loss: 0.6559005805424282, Train Acc: 63.93643031784841, Val Loss: 2478891.25, Val Acc: 36.26373626373626\n",
      "Epoch 97, Train Loss: 1917.8325847046715, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 98, Train Loss: 0.6558702502931867, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 99, Train Loss: 0.6581455980028424, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 100, Train Loss: 0.6546972138541085, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 101, Train Loss: 0.6570787685258048, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 102, Train Loss: 0.6548775008746556, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 103, Train Loss: 0.6668863722256252, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 104, Train Loss: 0.6593230962753296, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 105, Train Loss: 0.656573508466993, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 106, Train Loss: 0.6580186230795724, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 107, Train Loss: 0.6587975536073957, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 108, Train Loss: 0.6518415638378688, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 109, Train Loss: 0.651123012815203, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 110, Train Loss: 214.51442378759384, Train Acc: 62.34718826405868, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 111, Train Loss: 0.6561797601836068, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 112, Train Loss: 0.6663098761013576, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 113, Train Loss: 0.6522917406899589, Train Acc: 63.93643031784841, Val Loss: 2478891.25, Val Acc: 36.26373626373626\n",
      "Epoch 114, Train Loss: 0.6509353773934501, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 115, Train Loss: 0.6555267998150417, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 116, Train Loss: 0.6507205877985273, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 117, Train Loss: 0.6531400084495544, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 118, Train Loss: 0.6563111884253365, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 119, Train Loss: 0.6537059460367475, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 120, Train Loss: 0.6563526817730495, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 121, Train Loss: 0.6539015088762555, Train Acc: 63.93643031784841, Val Loss: 2478891.25, Val Acc: 36.26373626373626\n",
      "Epoch 122, Train Loss: 0.6569199391773769, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 123, Train Loss: 0.6594484959329877, Train Acc: 63.93643031784841, Val Loss: 2478891.25, Val Acc: 36.26373626373626\n",
      "Epoch 124, Train Loss: 0.6557439139911106, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 125, Train Loss: 0.6585193872451782, Train Acc: 63.93643031784841, Val Loss: 2478891.25, Val Acc: 36.26373626373626\n",
      "Epoch 126, Train Loss: 6022.365918423448, Train Acc: 62.10268948655257, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 127, Train Loss: 0.6516626902988979, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 128, Train Loss: 0.6656304001808167, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 129, Train Loss: 18663.85328162568, Train Acc: 61.98044009779951, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 130, Train Loss: 0.6607095769473484, Train Acc: 63.93643031784841, Val Loss: 2478891.0, Val Acc: 36.26373626373626\n",
      "Epoch 131, Train Loss: 2625.4772750479833, Train Acc: 63.93643031784841, Val Loss: 2478890.75, Val Acc: 36.26373626373626\n",
      "Epoch 132, Train Loss: 0.657057387488229, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 133, Train Loss: 0.6529384510857719, Train Acc: 63.93643031784841, Val Loss: 2478891.25, Val Acc: 36.26373626373626\n",
      "Epoch 134, Train Loss: 0.6610499535288129, Train Acc: 63.93643031784841, Val Loss: 2478891.25, Val Acc: 36.26373626373626\n",
      "Epoch 135, Train Loss: 0.6481248821531024, Train Acc: 63.93643031784841, Val Loss: 2478891.25, Val Acc: 36.26373626373626\n",
      "Epoch 136, Train Loss: 0.6503375853810992, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 137, Train Loss: 0.655208272593362, Train Acc: 63.93643031784841, Val Loss: 2478891.5, Val Acc: 36.26373626373626\n",
      "Epoch 138, Train Loss: 0.6588299785341535, Train Acc: 63.93643031784841, Val Loss: 2478891.25, Val Acc: 36.26373626373626\n",
      "Epoch 139, Train Loss: 0.6563922933169773, Train Acc: 63.93643031784841, Val Loss: 2478891.0, Val Acc: 36.26373626373626\n",
      "Epoch 140, Train Loss: 135.3903078351702, Train Acc: 62.10268948655257, Val Loss: 2478890.75, Val Acc: 36.26373626373626\n",
      "Epoch 141, Train Loss: 0.6534505571637835, Train Acc: 63.93643031784841, Val Loss: 2478891.0, Val Acc: 36.26373626373626\n",
      "Epoch 142, Train Loss: 0.6557541915348598, Train Acc: 63.93643031784841, Val Loss: 2478890.25, Val Acc: 36.26373626373626\n",
      "Epoch 143, Train Loss: 0.6589619772774833, Train Acc: 63.81418092909536, Val Loss: 2478890.75, Val Acc: 36.26373626373626\n",
      "Epoch 144, Train Loss: 0.6613229428018842, Train Acc: 63.93643031784841, Val Loss: 2478891.0, Val Acc: 36.26373626373626\n",
      "Epoch 145, Train Loss: 10151.072405474526, Train Acc: 63.93643031784841, Val Loss: 2478890.25, Val Acc: 36.26373626373626\n",
      "Epoch 146, Train Loss: 0.6518330829484122, Train Acc: 63.93643031784841, Val Loss: 2478891.0, Val Acc: 36.26373626373626\n",
      "Epoch 147, Train Loss: 0.6521652596337455, Train Acc: 63.93643031784841, Val Loss: 2478890.5, Val Acc: 36.26373626373626\n",
      "Epoch 148, Train Loss: 0.6491051742008754, Train Acc: 63.93643031784841, Val Loss: 2478891.0, Val Acc: 36.26373626373626\n",
      "Epoch 149, Train Loss: 0.6544299892016819, Train Acc: 63.93643031784841, Val Loss: 2478890.75, Val Acc: 36.26373626373626\n",
      "Epoch 150, Train Loss: 0.6526459796088082, Train Acc: 63.93643031784841, Val Loss: 2478890.25, Val Acc: 36.26373626373626\n",
      "Epoch 151, Train Loss: 0.659282888684954, Train Acc: 63.93643031784841, Val Loss: 2478890.75, Val Acc: 36.26373626373626\n",
      "Epoch 152, Train Loss: 0.6665896688188825, Train Acc: 63.93643031784841, Val Loss: 2478890.25, Val Acc: 36.26373626373626\n",
      "Epoch 153, Train Loss: 0.6571085282734462, Train Acc: 63.93643031784841, Val Loss: 2478890.25, Val Acc: 36.26373626373626\n",
      "Epoch 154, Train Loss: 508.81196816478456, Train Acc: 61.7359413202934, Val Loss: 2478890.25, Val Acc: 36.26373626373626\n",
      "Epoch 155, Train Loss: 0.6585824659892491, Train Acc: 63.93643031784841, Val Loss: 2478890.5, Val Acc: 36.26373626373626\n",
      "Epoch 156, Train Loss: 0.6566507475716727, Train Acc: 63.93643031784841, Val Loss: 2478890.75, Val Acc: 36.26373626373626\n",
      "Epoch 157, Train Loss: 0.6598490902355739, Train Acc: 63.93643031784841, Val Loss: 2478890.75, Val Acc: 36.26373626373626\n",
      "Epoch 158, Train Loss: 0.6532881345067706, Train Acc: 63.93643031784841, Val Loss: 2478891.0, Val Acc: 36.26373626373626\n",
      "Epoch 159, Train Loss: 0.6554401857512338, Train Acc: 63.93643031784841, Val Loss: 2478890.75, Val Acc: 36.26373626373626\n",
      "Epoch 160, Train Loss: 0.6625793831689017, Train Acc: 63.93643031784841, Val Loss: 2478890.75, Val Acc: 36.26373626373626\n",
      "Epoch 161, Train Loss: 0.6514780691691807, Train Acc: 63.93643031784841, Val Loss: 2478890.5, Val Acc: 36.26373626373626\n",
      "Epoch 162, Train Loss: 0.6587424618857247, Train Acc: 63.93643031784841, Val Loss: 2478890.5, Val Acc: 36.26373626373626\n",
      "Epoch 163, Train Loss: 8074.030280794416, Train Acc: 63.93643031784841, Val Loss: 2478890.75, Val Acc: 36.26373626373626\n",
      "Epoch 164, Train Loss: 0.6538913079670498, Train Acc: 63.93643031784841, Val Loss: 2478890.5, Val Acc: 36.26373626373626\n",
      "Epoch 165, Train Loss: 6310.752564379147, Train Acc: 63.93643031784841, Val Loss: 2478890.25, Val Acc: 36.26373626373626\n",
      "Epoch 166, Train Loss: 0.6586521863937378, Train Acc: 63.93643031784841, Val Loss: 2478888.75, Val Acc: 36.26373626373626\n",
      "Epoch 167, Train Loss: 0.6563455632754734, Train Acc: 63.93643031784841, Val Loss: 2478888.5, Val Acc: 36.26373626373626\n",
      "Epoch 168, Train Loss: 0.6526207412992205, Train Acc: 63.93643031784841, Val Loss: 2478888.75, Val Acc: 36.26373626373626\n",
      "Epoch 169, Train Loss: 14290.362475625107, Train Acc: 63.93643031784841, Val Loss: 2478888.5, Val Acc: 36.26373626373626\n",
      "Epoch 170, Train Loss: 0.6552645904677254, Train Acc: 63.93643031784841, Val Loss: 2478888.5, Val Acc: 36.26373626373626\n",
      "Epoch 171, Train Loss: 0.6576274718557086, Train Acc: 63.93643031784841, Val Loss: 2478888.5, Val Acc: 36.26373626373626\n",
      "Epoch 172, Train Loss: 0.6583581566810608, Train Acc: 63.93643031784841, Val Loss: 2478888.75, Val Acc: 36.26373626373626\n",
      "Epoch 173, Train Loss: 0.6465266517230442, Train Acc: 63.93643031784841, Val Loss: 2478888.75, Val Acc: 36.26373626373626\n",
      "Epoch 174, Train Loss: 293.36219377177105, Train Acc: 63.93643031784841, Val Loss: 2478888.75, Val Acc: 36.26373626373626\n",
      "Epoch 175, Train Loss: 0.66212911265237, Train Acc: 63.81418092909536, Val Loss: 2478889.0, Val Acc: 36.26373626373626\n",
      "Epoch 176, Train Loss: 0.6549421633992877, Train Acc: 63.93643031784841, Val Loss: 2478888.75, Val Acc: 36.26373626373626\n",
      "Epoch 177, Train Loss: 0.6599885054997036, Train Acc: 63.93643031784841, Val Loss: 2478888.5, Val Acc: 36.26373626373626\n",
      "Epoch 178, Train Loss: 0.6606417553765433, Train Acc: 63.93643031784841, Val Loss: 2478888.5, Val Acc: 36.26373626373626\n",
      "Epoch 179, Train Loss: 0.6534320797239032, Train Acc: 63.93643031784841, Val Loss: 2478888.5, Val Acc: 36.26373626373626\n",
      "Epoch 180, Train Loss: 0.6606326188359942, Train Acc: 63.93643031784841, Val Loss: 2478888.75, Val Acc: 36.26373626373626\n",
      "Epoch 181, Train Loss: 0.6619249667440142, Train Acc: 63.93643031784841, Val Loss: 2478888.5, Val Acc: 36.26373626373626\n",
      "Epoch 182, Train Loss: 0.6548642430986676, Train Acc: 63.93643031784841, Val Loss: 2478888.75, Val Acc: 36.26373626373626\n",
      "Epoch 183, Train Loss: 0.6601551260266986, Train Acc: 63.93643031784841, Val Loss: 2478888.75, Val Acc: 36.26373626373626\n",
      "Epoch 184, Train Loss: 0.6598977616855076, Train Acc: 63.93643031784841, Val Loss: 2478888.5, Val Acc: 36.26373626373626\n",
      "Epoch 185, Train Loss: 8938.543874578816, Train Acc: 63.93643031784841, Val Loss: 2478888.5, Val Acc: 36.26373626373626\n",
      "Epoch 186, Train Loss: 0.6552086642810276, Train Acc: 63.93643031784841, Val Loss: 2478889.0, Val Acc: 36.26373626373626\n",
      "Epoch 187, Train Loss: 0.6535927823611668, Train Acc: 63.93643031784841, Val Loss: 2478888.75, Val Acc: 36.26373626373626\n",
      "Epoch 188, Train Loss: 0.6543826801436288, Train Acc: 63.93643031784841, Val Loss: 2478888.75, Val Acc: 36.26373626373626\n",
      "Epoch 189, Train Loss: 0.6517481463296073, Train Acc: 63.93643031784841, Val Loss: 2478889.0, Val Acc: 36.26373626373626\n",
      "Epoch 190, Train Loss: 0.6556029319763184, Train Acc: 63.93643031784841, Val Loss: 2478888.75, Val Acc: 36.26373626373626\n",
      "Epoch 191, Train Loss: 0.6593056746891567, Train Acc: 63.93643031784841, Val Loss: 2478888.75, Val Acc: 36.26373626373626\n",
      "Epoch 192, Train Loss: 688.198236652783, Train Acc: 63.93643031784841, Val Loss: 2478888.75, Val Acc: 36.26373626373626\n",
      "Epoch 193, Train Loss: 0.6575002670288086, Train Acc: 63.93643031784841, Val Loss: 2478889.0, Val Acc: 36.26373626373626\n",
      "Epoch 194, Train Loss: 0.65711396081107, Train Acc: 63.93643031784841, Val Loss: 2478889.0, Val Acc: 36.26373626373626\n",
      "Epoch 195, Train Loss: 8779.764148669583, Train Acc: 63.93643031784841, Val Loss: 2478888.25, Val Acc: 36.26373626373626\n",
      "Epoch 196, Train Loss: 0.6604862298284259, Train Acc: 63.93643031784841, Val Loss: 2478888.5, Val Acc: 36.26373626373626\n",
      "Epoch 197, Train Loss: 0.6586488059588841, Train Acc: 63.93643031784841, Val Loss: 2478888.5, Val Acc: 36.26373626373626\n",
      "Epoch 198, Train Loss: 0.6500313196863446, Train Acc: 63.93643031784841, Val Loss: 2478888.5, Val Acc: 36.26373626373626\n",
      "Epoch 199, Train Loss: 135073.7069570252, Train Acc: 62.95843520782396, Val Loss: 2478889.0, Val Acc: 36.26373626373626\n",
      "Epoch 200, Train Loss: 0.6595921431268964, Train Acc: 63.93643031784841, Val Loss: 2478888.5, Val Acc: 36.26373626373626\n",
      "Epoch 201, Train Loss: 0.6594089950834002, Train Acc: 63.93643031784841, Val Loss: 2478885.0, Val Acc: 36.26373626373626\n",
      "Epoch 202, Train Loss: 0.6525923694883075, Train Acc: 63.93643031784841, Val Loss: 2478885.0, Val Acc: 36.26373626373626\n",
      "Epoch 203, Train Loss: 0.6547178030014038, Train Acc: 63.93643031784841, Val Loss: 2478885.25, Val Acc: 36.26373626373626\n",
      "Epoch 204, Train Loss: 0.657553323677608, Train Acc: 63.93643031784841, Val Loss: 2478885.0, Val Acc: 36.26373626373626\n",
      "Epoch 205, Train Loss: 0.6604172587394714, Train Acc: 63.93643031784841, Val Loss: 2478885.0, Val Acc: 36.26373626373626\n",
      "Epoch 206, Train Loss: 0.6556427904537746, Train Acc: 63.93643031784841, Val Loss: 2478885.0, Val Acc: 36.26373626373626\n",
      "Epoch 207, Train Loss: 0.6673435143062046, Train Acc: 63.93643031784841, Val Loss: 2478884.75, Val Acc: 36.26373626373626\n",
      "Epoch 208, Train Loss: 0.6571287768227714, Train Acc: 63.93643031784841, Val Loss: 2478885.0, Val Acc: 36.26373626373626\n",
      "Epoch 209, Train Loss: 0.6480733411652702, Train Acc: 63.93643031784841, Val Loss: 2478885.25, Val Acc: 36.26373626373626\n",
      "Epoch 210, Train Loss: 0.6571453724588666, Train Acc: 63.93643031784841, Val Loss: 2478885.25, Val Acc: 36.26373626373626\n",
      "Epoch 211, Train Loss: 0.6549204587936401, Train Acc: 63.93643031784841, Val Loss: 2478884.75, Val Acc: 36.26373626373626\n",
      "Epoch 212, Train Loss: 0.6614504626819065, Train Acc: 63.93643031784841, Val Loss: 2478884.75, Val Acc: 36.26373626373626\n",
      "Epoch 213, Train Loss: 0.6567471623420715, Train Acc: 63.93643031784841, Val Loss: 2478885.0, Val Acc: 36.26373626373626\n",
      "Epoch 214, Train Loss: 0.6530333246503558, Train Acc: 63.93643031784841, Val Loss: 2478885.0, Val Acc: 36.26373626373626\n",
      "Epoch 215, Train Loss: 0.6625125408172607, Train Acc: 63.93643031784841, Val Loss: 2478885.0, Val Acc: 36.26373626373626\n",
      "Epoch 216, Train Loss: 0.6614993384906224, Train Acc: 63.93643031784841, Val Loss: 2478884.75, Val Acc: 36.26373626373626\n",
      "Epoch 217, Train Loss: 8066.190247493131, Train Acc: 63.93643031784841, Val Loss: 2478885.0, Val Acc: 36.26373626373626\n",
      "Epoch 218, Train Loss: 0.6628747156688145, Train Acc: 63.93643031784841, Val Loss: 2478885.25, Val Acc: 36.26373626373626\n",
      "Epoch 219, Train Loss: 0.6540527173451015, Train Acc: 63.93643031784841, Val Loss: 2478885.0, Val Acc: 36.26373626373626\n",
      "Epoch 220, Train Loss: 0.6596560563359942, Train Acc: 63.93643031784841, Val Loss: 2478885.25, Val Acc: 36.26373626373626\n",
      "Epoch 221, Train Loss: 1008.415210877146, Train Acc: 61.613691931540345, Val Loss: 2478882.75, Val Acc: 36.26373626373626\n",
      "Epoch 222, Train Loss: 0.6566119619778225, Train Acc: 63.93643031784841, Val Loss: 2478882.75, Val Acc: 36.26373626373626\n",
      "Epoch 223, Train Loss: 0.6609898975917271, Train Acc: 63.20293398533008, Val Loss: 2478882.75, Val Acc: 36.26373626373626\n",
      "Epoch 224, Train Loss: 0.6644194637026105, Train Acc: 63.93643031784841, Val Loss: 2478882.75, Val Acc: 36.26373626373626\n",
      "Epoch 225, Train Loss: 0.6569847464561462, Train Acc: 63.93643031784841, Val Loss: 2478882.75, Val Acc: 36.26373626373626\n",
      "Epoch 226, Train Loss: 2494.0698437605583, Train Acc: 62.95843520782396, Val Loss: 2478882.75, Val Acc: 36.26373626373626\n",
      "Epoch 227, Train Loss: 0.6531084179878235, Train Acc: 63.93643031784841, Val Loss: 2478882.75, Val Acc: 36.26373626373626\n",
      "Epoch 228, Train Loss: 0.655304593699319, Train Acc: 63.93643031784841, Val Loss: 2478882.75, Val Acc: 36.26373626373626\n",
      "Epoch 229, Train Loss: 0.6478222864014762, Train Acc: 63.93643031784841, Val Loss: 2478882.75, Val Acc: 36.26373626373626\n",
      "Epoch 230, Train Loss: 0.6508634260722569, Train Acc: 63.93643031784841, Val Loss: 2478882.75, Val Acc: 36.26373626373626\n",
      "Epoch 231, Train Loss: 3642.7566007120267, Train Acc: 63.93643031784841, Val Loss: 2478882.75, Val Acc: 36.26373626373626\n",
      "Epoch 232, Train Loss: 0.6588685427393232, Train Acc: 63.93643031784841, Val Loss: 2478882.75, Val Acc: 36.26373626373626\n",
      "Epoch 233, Train Loss: 0.6585473333086286, Train Acc: 63.93643031784841, Val Loss: 2478882.75, Val Acc: 36.26373626373626\n",
      "Epoch 234, Train Loss: 0.6548914739063808, Train Acc: 63.93643031784841, Val Loss: 2478882.75, Val Acc: 36.26373626373626\n",
      "Epoch 235, Train Loss: 0.6560309273856026, Train Acc: 63.93643031784841, Val Loss: 2478882.75, Val Acc: 36.26373626373626\n",
      "Epoch 236, Train Loss: 6472.055807266916, Train Acc: 62.59168704156479, Val Loss: 2478882.75, Val Acc: 36.26373626373626\n",
      "Epoch 237, Train Loss: 0.6535095998219081, Train Acc: 63.93643031784841, Val Loss: 2478882.75, Val Acc: 36.26373626373626\n",
      "Epoch 238, Train Loss: 0.6517753601074219, Train Acc: 63.93643031784841, Val Loss: 2478882.75, Val Acc: 36.26373626373626\n",
      "Epoch 239, Train Loss: 0.6591554028647286, Train Acc: 63.93643031784841, Val Loss: 2478882.75, Val Acc: 36.26373626373626\n",
      "Epoch 240, Train Loss: 0.6552549600601196, Train Acc: 63.93643031784841, Val Loss: 2478882.75, Val Acc: 36.26373626373626\n",
      "Epoch 241, Train Loss: 0.6586299708911351, Train Acc: 63.93643031784841, Val Loss: 2478882.75, Val Acc: 36.26373626373626\n",
      "Epoch 242, Train Loss: 0.6584780897412982, Train Acc: 63.93643031784841, Val Loss: 2478882.75, Val Acc: 36.26373626373626\n",
      "Epoch 243, Train Loss: 0.6584013445036752, Train Acc: 63.93643031784841, Val Loss: 2478882.75, Val Acc: 36.26373626373626\n",
      "Epoch 244, Train Loss: 0.6446018985339573, Train Acc: 63.93643031784841, Val Loss: 2478882.75, Val Acc: 36.26373626373626\n",
      "Epoch 245, Train Loss: 0.6538385834012713, Train Acc: 63.93643031784841, Val Loss: 2478883.0, Val Acc: 36.26373626373626\n",
      "Epoch 246, Train Loss: 0.6543370144707816, Train Acc: 63.93643031784841, Val Loss: 2478882.75, Val Acc: 36.26373626373626\n",
      "Epoch 247, Train Loss: 18651.44658921446, Train Acc: 63.93643031784841, Val Loss: 2478882.75, Val Acc: 36.26373626373626\n",
      "Epoch 248, Train Loss: 0.6551610486848014, Train Acc: 63.93643031784841, Val Loss: 2478882.75, Val Acc: 36.26373626373626\n",
      "Epoch 249, Train Loss: 0.6584964735167367, Train Acc: 63.93643031784841, Val Loss: 2478882.75, Val Acc: 36.26373626373626\n",
      "Epoch 250, Train Loss: 0.6601206745420184, Train Acc: 63.93643031784841, Val Loss: 2478882.75, Val Acc: 36.26373626373626\n"
     ]
    }
   ],
   "source": [
    "# train.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import json\n",
    "from data.data_loader import load_and_align_data, create_data_loader\n",
    "from models.model import CombinedGAT,CombinedGCN\n",
    "from utils import EarlyStopping  # 假设你有评估和早停的辅助函数\n",
    "from torch_geometric.data import Data\n",
    "import logging\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "\n",
    "\n",
    "# 加载配置文件\n",
    "#with open('models/config.json', 'r') as config_file:\n",
    "#    config = json.load(config_file)\n",
    "\n",
    "# 设置训练设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 添加日志记录功能\n",
    "logging.basicConfig(filename='training.log', level=logging.INFO, \n",
    "                    format='%(asctime)s:%(levelname)s:%(message)s')\n",
    "\n",
    "# 加载数据并创建数据集\n",
    "train_dataset, val_dataset, _ = load_and_align_data(high_dim_path = config['data']['high_dim_path'], \n",
    "                                                  low_dim_path = config['data']['low_dim_path'],\n",
    "                                                  labels_path = config['data']['labels_path'],\n",
    "                                                  test_size = config['data']['test_size'],\n",
    "                                                  val_size = config['data']['val_size'],\n",
    "                                                  random_state = config['data']['random_state'])\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = create_data_loader(train_dataset, batch_size=config['data']['batch_size'], shuffle=config['data']['shuffle'])\n",
    "val_loader = create_data_loader(val_dataset, batch_size=config['data']['batch_size'], shuffle=config['data']['shuffle'])\n",
    "\n",
    "model = CombinedGAT(high_dim_input_size=config[\"model\"][\"high_dim_input_size\"],  # 适当调整这些参数，这里可以写成config\n",
    "                 low_dim_input_size=config[\"model\"][\"low_dim_input_size\"],\n",
    "                 embedding_dim=config[\"model\"][\"embedding_dim\"],\n",
    "                 output_dim=config[\"model\"][\"output_dim\"],  # 根据您的任务调整\n",
    "                 hidden_channels=config[\"model\"][\"hidden_channels\"],\n",
    "                 num_heads=config[\"model\"][\"num_heads\"]).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config['train']['learning_rate'])\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5)\n",
    "\n",
    "# 初始化早停对象\n",
    "early_stopping = EarlyStopping(patience=config[\"earlystopping\"][\"patience\"], delta=config[\"earlystopping\"][\"delta\"])\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(config['train']['epochs']):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for high_dim_features, low_dim_features, labels in train_loader: # , edge_index in train_loader:\n",
    "        \n",
    "        batch_size = high_dim_features.size(0)  # 获取当前批次的大小\n",
    "        \n",
    "        # 为当前批次生成全连接的邻接矩阵\n",
    "        adj_matrix = torch.ones((batch_size, batch_size)) - torch.eye(batch_size)\n",
    "        edge_index, _ = dense_to_sparse(adj_matrix)\n",
    "        \n",
    "        \n",
    "        # 准备数据\n",
    "        high_dim_features = high_dim_features.to(device)\n",
    "        low_dim_features = low_dim_features.to(device)\n",
    "        labels = labels.to(device)\n",
    "        edge_index = edge_index.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(high_dim_features, low_dim_features, edge_index)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    train_acc = 100 * correct / total\n",
    "\n",
    "    # 计算验证集上的损失\n",
    "    model.eval()\n",
    "    val_total_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for high_dim_features, low_dim_features, labels in val_loader: # ,edge_index in val_loader:  # 假设你有一个验证集加载器val_loader\n",
    "            \n",
    "            batch_size = high_dim_features.size(0)  # 获取当前批次的大小\n",
    "        \n",
    "            # 为当前批次生成全连接的邻接矩阵\n",
    "            adj_matrix = torch.ones((batch_size, batch_size)) - torch.eye(batch_size)\n",
    "            edge_index, _ = dense_to_sparse(adj_matrix)\n",
    "                \n",
    "            high_dim_features = high_dim_features.to(device)\n",
    "            low_dim_features = low_dim_features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            edge_index = edge_index.to(device)\n",
    "\n",
    "            outputs = model(high_dim_features, low_dim_features, edge_index)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    val_loss =  val_total_loss/len(val_loader)\n",
    "    val_acc = 100 * val_correct / val_total\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    logging.info(f'Epoch {epoch+1}, Train Loss: {train_loss}, Train Acc: {train_acc}, Val Loss: {val_loss}, Val Acc: {val_acc}')\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss}, Train Acc: {train_acc}, Val Loss: {val_loss}, Val Acc: {val_acc}')\n",
    "    \n",
    "    # 早停检查和保存最佳模型\n",
    "#    early_stopping(val_loss,model)\n",
    "#    if early_stopping.early_stop:\n",
    "#        print(\"Early stopping\")\n",
    "#        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c29abf5-0af5-4e7f-a733-e81aaf25104d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
